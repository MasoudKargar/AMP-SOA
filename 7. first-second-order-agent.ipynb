{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yv3IDvrobU37"
   },
   "source": [
    "# AMP-SOA portfolio optimization using adaptive meta-policy based on second-order agents with deep reinforcment learning\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4kHCfEiTA80V"
   },
   "source": [
    "## 7.0 Deep Reinforcement Learning Portfolios (First order agents) and second order agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "12v1i0jVkg48"
   },
   "source": [
    "### 7.1 Important var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#second-order-agent-days-neuralnetwork\n",
    "days=10\n",
    "neural_network = 'dense_bn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ntfTb0e2bU4C",
    "outputId": "d268ef40-e425-4671-f1ec-e9e5d6b65659",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pandas\n",
    "import numpy as np\n",
    "import numpy\n",
    "import matplotlib\n",
    "import math\n",
    "import models\n",
    "import os\n",
    "matplotlib.use('Agg')\n",
    "import datetime\n",
    "from pypfopt import expected_returns, risk_models, EfficientFrontier, objective_functions\n",
    "from config import config\n",
    "from backtest import backtest_strat, baseline_strat\n",
    "import env_portfolio\n",
    "from env_portfolio import StockPortfolioEnv\n",
    "from models import DRLAgent\n",
    "from finrl.preprocessing.data import data_split\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from keras.layers import Dense, LSTM, LeakyReLU, Dropout, Conv2D, MaxPooling2D, Flatten, Conv1D, MaxPooling1D\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import RobustScaler,StandardScaler\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n",
    "import seaborn as sns\n",
    "from keras.optimizers import SGD\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import pyfolio as pf\n",
    "from pyfolio import timeseries\n",
    "from pypfopt.efficient_frontier import EfficientFrontier\n",
    "from pypfopt import risk_models\n",
    "from pypfopt import expected_returns\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "best_return = True\n",
    "while best_return: \n",
    "    tech_indicator_list = ['f01','f02','f03','f04']\n",
    "    %store -r train_df\n",
    "    %store -r test_df\n",
    "    train_df.head()\n",
    "    stock_dimension = len(train_df.tic.unique())\n",
    "    state_space = stock_dimension\n",
    "    print(f\"Stock Dimension: {stock_dimension}, State Space: {state_space}\")\n",
    "    weights_initial = [1/stock_dimension]*stock_dimension\n",
    "    env_kwargs = {\n",
    "        \"hmax\": 500, \n",
    "        \"initial_amount\": 1000000, \n",
    "        \"transaction_cost_pct\": 0.001, \n",
    "        \"state_space\": state_space, \n",
    "        \"stock_dim\": stock_dimension, \n",
    "        \"tech_indicator_list\": tech_indicator_list, \n",
    "        \"action_space\": stock_dimension, \n",
    "        \"reward_scaling\": 0,\n",
    "        'initial_weights': [1/stock_dimension]*stock_dimension\n",
    "    }\n",
    "    e_train_gym = StockPortfolioEnv(df = train_df, **env_kwargs)\n",
    "    env_train, _ = e_train_gym.get_sb_env()\n",
    "    print(type(env_train))\n",
    "    \n",
    "    # 1) DEFITION AGENTS\n",
    "    ####A2C AGENT\n",
    "    agent = DRLAgent(env = env_train)\n",
    "    A2C_PARAMS = {\"n_steps\": 5, \"ent_coef\": 0.005, \"learning_rate\": 0.001}\n",
    "    model_a2c = agent.get_model(model_name=\"a2c\",model_kwargs = A2C_PARAMS)\n",
    "    trained_a2c = agent.train_model(model=model_a2c,    tb_log_name='a2c',   total_timesteps=10000)\n",
    "    agent = DRLAgent(env = env_train)\n",
    "    PPO_PARAMS = {\n",
    "        \"n_steps\": 2048,\n",
    "        \"ent_coef\": 0.005,\n",
    "        \"learning_rate\": 0.001,\n",
    "        \"batch_size\": 100,\n",
    "    }\n",
    "    model_ppo = agent.get_model(\"ppo\",model_kwargs = PPO_PARAMS)\n",
    "    trained_ppo = agent.train_model(model=model_ppo,   tb_log_name='ppo',  total_timesteps=10000)\n",
    "    ####DDPG AGENT\n",
    "    agent = DRLAgent(env = env_train)\n",
    "    DDPG_PARAMS = {\"batch_size\": 100, \"buffer_size\": 50000, \"learning_rate\": 0.001,\"learning_starts\": 100}\n",
    "    model_ddpg = agent.get_model(\"ddpg\",model_kwargs = DDPG_PARAMS)\n",
    "    trained_ddpg = agent.train_model(model=model_ddpg,  tb_log_name='ddpg', total_timesteps=10000)\n",
    "    ####TD3 AGENT\n",
    "    agent = DRLAgent(env=env_train)\n",
    "    TD3_PARAMS = {\"batch_size\": 100,\n",
    "                \"buffer_size\": 50000,\n",
    "                \"learning_rate\": 0.001,\n",
    "                \"learning_starts\": 100}\n",
    "    model_td3 = agent.get_model(\"td3\", model_kwargs=TD3_PARAMS)\n",
    "    trained_td3 = agent.train_model(\n",
    "        model=model_td3, tb_log_name='td3', total_timesteps=10000)\n",
    "    ####SAC AGENT\n",
    "    agent = DRLAgent(env=env_train)\n",
    "    SAC_PARAMS = {\n",
    "        \"batch_size\": 100,\n",
    "        \"buffer_size\": 50000,\n",
    "        \"learning_rate\": 0.001,\n",
    "        \"learning_starts\": 100,\n",
    "        \"ent_coef\": \"auto_0.1\",\n",
    "    }\n",
    "    model_sac = agent.get_model(\"sac\", model_kwargs=SAC_PARAMS)\n",
    "    trained_sac = agent.train_model(\n",
    "        model=model_sac, tb_log_name='sac', total_timesteps=10000)\n",
    "    \n",
    "    # 2)Train the agents\n",
    "    # A2C Train Model\n",
    "    e_trade_gym = StockPortfolioEnv(df = train_df, **env_kwargs)\n",
    "    env_trade, obs_trade = e_trade_gym.get_sb_env()\n",
    "    a2c_train_daily_return, a2c_train_weights = DRLAgent.DRL_prediction(model=trained_a2c, test_data = train_df, test_env = env_trade, test_obs = obs_trade)\n",
    "    # PPO Train Model\n",
    "    e_trade_gym = StockPortfolioEnv(df = train_df, **env_kwargs)\n",
    "    env_trade, obs_trade = e_trade_gym.get_sb_env()\n",
    "    ppo_train_daily_return, ppo_train_weights = DRLAgent.DRL_prediction(model=trained_ppo, test_data = train_df,test_env = env_trade,test_obs = obs_trade)\n",
    "    # DDPG Train Model\n",
    "    e_trade_gym = StockPortfolioEnv(df = train_df, **env_kwargs)\n",
    "    env_trade, obs_trade = e_trade_gym.get_sb_env()\n",
    "    ddpg_train_daily_return, ddpg_train_weights = DRLAgent.DRL_prediction(model=trained_ddpg, test_data = train_df,  test_env = env_trade, test_obs = obs_trade)\n",
    "    # TD3 Train Model\n",
    "    e_trade_gym = StockPortfolioEnv(df=train_df, **env_kwargs)\n",
    "    env_trade, obs_trade = e_trade_gym.get_sb_env()\n",
    "    td3_train_daily_return, td3_train_weights = DRLAgent.DRL_prediction(\n",
    "        model=trained_td3, test_data=train_df,  test_env=env_trade, test_obs=obs_trade)\n",
    "    # SAC Train Model\n",
    "    e_trade_gym = StockPortfolioEnv(df=train_df, **env_kwargs)\n",
    "    env_trade, obs_trade = e_trade_gym.get_sb_env()\n",
    "    sac_train_daily_return, sac_train_weights = DRLAgent.DRL_prediction(\n",
    "        model=trained_sac, test_data=train_df,  test_env=env_trade, test_obs=obs_trade)\n",
    "    # Store the Training Models\n",
    "    %store a2c_train_daily_return\n",
    "    %store ppo_train_daily_return\n",
    "    %store ddpg_train_daily_return\n",
    "    %store td3_train_daily_return\n",
    "    %store sac_train_daily_return\n",
    "    a2c_train_daily_return.to_csv('a2c_train_daily_return.csv',index=False)\n",
    "    ppo_train_daily_return.to_csv('ppo_train_daily_return.csv',index=False)\n",
    "    ddpg_train_daily_return.to_csv('ddpg_train_daily_return.csv',index=False)\n",
    "    td3_train_daily_return.to_csv('td3_train_daily_return.csv',index=False)\n",
    "    sac_train_daily_return.to_csv('sac_train_daily_return.csv',index=False)\n",
    "\n",
    "    # 3) Trading (test Agents)\n",
    "    # A2C Test Model\n",
    "    e_trade_gym = StockPortfolioEnv(df = test_df, **env_kwargs)\n",
    "    env_trade, obs_trade = e_trade_gym.get_sb_env()\n",
    "    a2c_test_daily_return, a2c_test_weights = DRLAgent.DRL_prediction(model=trained_a2c, test_data = test_df, test_env = env_trade, test_obs = obs_trade)\n",
    "    a2c_test_weights.to_csv('a2c_test_weights.csv')\n",
    "    a2c_test_daily_return.to_csv('a2c_test_daily_return.csv')\n",
    "    a2c_test_weights\n",
    "    # PPO Test Model\n",
    "    e_trade_gym = StockPortfolioEnv(df = test_df, **env_kwargs)\n",
    "    env_trade, obs_trade = e_trade_gym.get_sb_env()\n",
    "    ppo_test_daily_return, ppo_test_weights = DRLAgent.DRL_prediction(model=trained_ppo, test_data = test_df, test_env = env_trade, test_obs = obs_trade)\n",
    "    ppo_test_weights.to_csv('ppo_test_weights.csv')\n",
    "    ppo_test_daily_return.to_csv('ppo_test_daily_return.csv')\n",
    "    # DDPG Test Model\n",
    "    e_trade_gym = StockPortfolioEnv(df = test_df, **env_kwargs)\n",
    "    env_trade, obs_trade = e_trade_gym.get_sb_env()\n",
    "    ddpg_test_daily_return, ddpg_test_weights = DRLAgent.DRL_prediction(model=trained_ddpg, test_data = test_df, test_env = env_trade,test_obs = obs_trade)\n",
    "    ddpg_test_weights.to_csv('ddpg_test_weights.csv')\n",
    "    ddpg_test_daily_return.to_csv('ddpg_test_daily_return.csv')\n",
    "    # TD3 Test Model\n",
    "    e_trade_gym = StockPortfolioEnv(df=test_df, **env_kwargs)\n",
    "    env_trade, obs_trade = e_trade_gym.get_sb_env()\n",
    "    td3_test_daily_return, td3_test_weights = DRLAgent.DRL_prediction(\n",
    "        model=trained_td3, test_data=test_df, test_env=env_trade, test_obs=obs_trade)\n",
    "    td3_test_weights.to_csv('td3_test_weights.csv')\n",
    "    td3_test_daily_return.to_csv('td3_test_daily_return.csv')\n",
    "    # SAC Test Model\n",
    "    e_trade_gym = StockPortfolioEnv(df=test_df, **env_kwargs)\n",
    "    env_trade, obs_trade = e_trade_gym.get_sb_env()\n",
    "    sac_test_daily_return, sac_test_weights = DRLAgent.DRL_prediction(\n",
    "        model=trained_sac, test_data=test_df, test_env=env_trade, test_obs=obs_trade)\n",
    "    sac_test_weights.to_csv('sac_test_weights.csv')\n",
    "    sac_test_daily_return.to_csv('sac_test_daily_return.csv')\n",
    "    ### Save the Portfolios\n",
    "    a2c_test_portfolio = a2c_test_weights.copy()\n",
    "    a2c_test_returns = a2c_test_daily_return.copy()\n",
    "    ppo_test_portfolio = ppo_test_weights.copy()\n",
    "    ppo_test_returns = ppo_test_daily_return.copy()\n",
    "    ddpg_test_portfolio = ddpg_test_weights.copy()\n",
    "    ddpg_test_returns = ddpg_test_daily_return.copy()\n",
    "    td3_test_portfolio = td3_test_weights.copy()\n",
    "    td3_test_returns = td3_test_daily_return.copy()\n",
    "    sac_test_portfolio = sac_test_weights.copy()\n",
    "    sac_test_returns = sac_test_daily_return.copy()\n",
    "    %store a2c_test_portfolio\n",
    "    %store a2c_test_returns \n",
    "    %store ppo_test_portfolio\n",
    "    %store ppo_test_returns \n",
    "    %store ddpg_test_portfolio\n",
    "    %store ddpg_test_returns\n",
    "    %store td3_test_portfolio\n",
    "    %store td3_test_returns\n",
    "    %store sac_test_portfolio\n",
    "    %store sac_test_returns\n",
    "    \n",
    "    # 4) Second order agent init\n",
    "    agents_count = 5\n",
    "    a2c_test_weights = pd.read_csv('a2c_test_weights.csv')\n",
    "    ppo_test_weights = pd.read_csv('ppo_test_weights.csv')\n",
    "    ddpg_test_weights = pd.read_csv('ddpg_test_weights.csv')\n",
    "    td3_test_weights = pd.read_csv('td3_test_weights.csv')\n",
    "    sac_test_weights = pd.read_csv('sac_test_weights.csv')\n",
    "    df_close_full_stocks = pd.read_csv('datasets/close_prices.csv')\n",
    "    %store -r filtered_stocks\n",
    "    # %store -r df_close_full_stocks\n",
    "    a2c_test_weights_dropdate = a2c_test_weights.drop(columns=['date'])\n",
    "    ppo_test_weights_dropdate = ppo_test_weights.drop(columns=['date'])\n",
    "    ddpg_test_weights_dropdate = ddpg_test_weights.drop(columns=['date'])\n",
    "    td3_test_weights_dropdate = td3_test_weights.drop(columns=['date'])\n",
    "    sac_test_weights_dropdate = sac_test_weights.drop(columns=['date'])\n",
    "    all_test_weights = a2c_test_weights_dropdate + ppo_test_weights_dropdate + \\\n",
    "        ddpg_test_weights_dropdate + td3_test_weights_dropdate + sac_test_weights_dropdate\n",
    "    proposed_method1_agents_test_weights_avg = all_test_weights / agents_count\n",
    "    proposed_method1_agents_test_weights_avg.to_csv('proposed_method1_agents_test_weights_avg.csv', index=False)\n",
    "    %store proposed_method1_agents_test_weights_avg\n",
    "\n",
    "    start_date = '2021-03-29'\n",
    "    end_date = '2024-03-28'\n",
    "    filtered_df = df_close_full_stocks[(df_close_full_stocks['date'] < end_date) & (df_close_full_stocks['date'] >= start_date)]\n",
    "    filtered_df.reset_index(drop=True, inplace=True)\n",
    "    columns_to_drop = [col for col in filtered_df if col not in filtered_stocks]\n",
    "    df_kept = filtered_df.drop(columns=columns_to_drop)\n",
    "    df1 = df_kept.reindex(sorted(df_kept.columns), axis=1)\n",
    "    test_close_normal = df1.pct_change()\n",
    "    proposed_method1_eachstock_return = test_close_normal * proposed_method1_agents_test_weights_avg\n",
    "    proposed_method1_eachstock_return.to_csv('proposed_method1_eachstock_return.csv', index=False)\n",
    "\n",
    "    # Initialize a list to store row sums\n",
    "    row_sums = []\n",
    "\n",
    "    # Calculate row sums and store them\n",
    "    for index, row in proposed_method1_eachstock_return.iterrows():\n",
    "        row_sum = row.sum()\n",
    "        row_sums.append(row_sum)\n",
    "\n",
    "    # Create a new DataFrame with row sums\n",
    "    proposed_method1_test_daily_return = pd.DataFrame({'daily_return': row_sums})\n",
    "    proposed_method1_test_daily_return.insert(1, 'date', a2c_test_weights['date'])\n",
    "    proposed_method1_test_daily_return = proposed_method1_test_daily_return[['date','daily_return']]\n",
    "    proposed_method1_test_daily_return.to_csv('proposed_method1_test_daily_return.csv', index=False)\n",
    "    %store proposed_method1_test_daily_return\n",
    "\n",
    "    \n",
    "    def load_and_merge_daily_returns(filenames, output_filename):\n",
    "        dfs = []\n",
    "        \n",
    "        # Load each CSV file, drop the 'date' column, and store the DataFrame in a list\n",
    "        for col_name, filename in filenames.items():\n",
    "            df = pd.read_csv(filename)\n",
    "            df = df.drop(columns=['date'])\n",
    "            df.columns = [col_name]\n",
    "            dfs.append(df)\n",
    "        \n",
    "        # Concatenate all DataFrames along the columns\n",
    "        merged_df = pd.concat(dfs, axis=1)\n",
    "        \n",
    "        # Save the merged DataFrame to a CSV file\n",
    "        merged_df.to_csv(output_filename, index=False)\n",
    "        \n",
    "        return merged_df\n",
    "\n",
    "    # Dictionary of filenames and corresponding column names\n",
    "    filenames = {\n",
    "        'a2c': 'a2c_train_daily_return.csv',\n",
    "        'ppo': 'ppo_train_daily_return.csv',\n",
    "        'ddpg': 'ddpg_train_daily_return.csv',\n",
    "        'td3': 'td3_train_daily_return.csv',\n",
    "        'sac': 'sac_train_daily_return.csv'\n",
    "    }\n",
    "\n",
    "    # Output filename\n",
    "    output_filename = 'merged_train_daily_return.csv'\n",
    "\n",
    "    # Call the function and get the merged DataFrame\n",
    "    merged_train_daily_return = load_and_merge_daily_returns(filenames, output_filename)\n",
    "\n",
    "    merged_train_daily_return.to_csv('merged_train_daily_return.csv')\n",
    "\n",
    "    merged_train_daily_return.plot(figsize=(10, 6))\n",
    "    plt.title('Line Plot')\n",
    "    plt.xlabel('X Axis Label')\n",
    "    plt.ylabel('Y Axis Label')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    def load_and_merge_test_returns(file_dict, output_filename):\n",
    "        dfs = []\n",
    "        \n",
    "        # Load each CSV file, drop the 'Unnamed: 0' and 'date' columns, and store the DataFrame in a list\n",
    "        for col_name, filename in file_dict.items():\n",
    "            df = pd.read_csv(filename)\n",
    "            if 'Unnamed: 0' in df.columns:\n",
    "                df = df.drop(columns=['Unnamed: 0'])\n",
    "            if 'date' in df.columns:\n",
    "                df = df.drop(columns=['date'])\n",
    "            df.columns = [col_name]\n",
    "            dfs.append(df)\n",
    "        \n",
    "        # Concatenate all DataFrames along the columns\n",
    "        merged_df = pd.concat(dfs, axis=1)\n",
    "        \n",
    "        # Save the merged DataFrame to a CSV file\n",
    "        merged_df.to_csv(output_filename, index=False)\n",
    "        \n",
    "        return merged_df\n",
    "\n",
    "    # Dictionary of filenames and corresponding column names\n",
    "    file_dict = {\n",
    "        'a2c': 'a2c_test_daily_return.csv',\n",
    "        'ppo': 'ppo_test_daily_return.csv',\n",
    "        'ddpg': 'ddpg_test_daily_return.csv',\n",
    "        'td3': 'td3_test_daily_return.csv',\n",
    "        'sac': 'sac_test_daily_return.csv'\n",
    "    }\n",
    "\n",
    "    # Output filename\n",
    "    output_filename = 'merged_test_daily_return.csv'\n",
    "\n",
    "    # Call the function and get the merged DataFrame\n",
    "    merged_test_daily_return = load_and_merge_test_returns(file_dict, output_filename)\n",
    "\n",
    "    merged_test_daily_return.to_csv('merged_test_daily_return.csv')\n",
    "\n",
    "    merged_test_daily_return.plot(figsize=(10, 6))\n",
    "    plt.title('Line Plot')\n",
    "    plt.xlabel('X Axis Label')\n",
    "    plt.ylabel('Y Axis Label')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    columns_to_sum = ['a2c', 'ppo', 'ddpg', 'td3', 'sac']\n",
    "\n",
    "    # Calculate ratios for each column in columns_to_sum\n",
    "    for column in columns_to_sum:\n",
    "        for i in range(0, len(merged_train_daily_return) - days + 1):\n",
    "            sum_rows_allcolumns = merged_train_daily_return[columns_to_sum].iloc[i:i + days].sum().sum()\n",
    "            sum_rows = merged_train_daily_return[column].iloc[i:i + days].sum()\n",
    "            if sum_rows_allcolumns < 0.0005:\n",
    "                sum_rows_allcolumns*=10\n",
    "                ratio = sum_rows \n",
    "            else:\n",
    "                ratio = sum_rows\n",
    "            if i + days < len(merged_train_daily_return):\n",
    "                merged_train_daily_return.loc[i + days, f'sumavg_{column}'] = ratio\n",
    "\n",
    "    # Save the updated DataFrame to a CSV file\n",
    "    merged_train_daily_return.to_csv('train_sum_avg.csv', index=False)\n",
    "    merged_train_daily_return\n",
    "\n",
    "    columns_to_sum = ['a2c', 'ppo', 'ddpg', 'td3', 'sac']\n",
    "\n",
    "    # Calculate ratios for each column in columns_to_sum\n",
    "    for column in columns_to_sum:\n",
    "        for i in range(0, len(merged_test_daily_return)):\n",
    "            sum_rows_allcolumns = merged_test_daily_return[columns_to_sum].iloc[i:i+days].sum().sum()\n",
    "            sum_rows = merged_test_daily_return[column].iloc[i:i+days].sum()\n",
    "            if sum_rows_allcolumns < 0.0005:\n",
    "                sum_rows_allcolumns*=10\n",
    "                ratio = sum_rows \n",
    "            else:\n",
    "                ratio = sum_rows \n",
    "            #ratio = sum_rows / sum_rows_allcolumns if sum_rows_allcolumns != 0 else 0\n",
    "            if i + days < len(merged_test_daily_return):\n",
    "                merged_test_daily_return.loc[i + days, f'sumavg_{column}'] = ratio\n",
    "\n",
    "    # Save the updated DataFrame to a CSV file\n",
    "    merged_test_daily_return.to_csv('test_sum_avg.csv', index=False)\n",
    "    merged_test_daily_return\n",
    "\n",
    "    merged_train_daily_return_x = merged_train_daily_return.iloc[:, 0:5]\n",
    "    merged_test_daily_return_x = merged_test_daily_return.iloc[:, 0:5]\n",
    "\n",
    "    merged_train_daily_return_x.to_csv('merged_train_daily_return_x.csv')\n",
    "    merged_train_daily_return_x\n",
    "\n",
    "    merged_test_daily_return_x.to_csv('merged_test_daily_return_x.csv')\n",
    "    merged_test_daily_return_x\n",
    "    cumulative_rows = []\n",
    "    for i in range(0, len(merged_train_daily_return_x)-days, 1):\n",
    "        group = merged_train_daily_return_x.iloc[i:i+days]\n",
    "        cumulative_row = group.iloc[0].tolist()\n",
    "        for j in range(1, len(group)):\n",
    "            cumulative_row.extend(group.iloc[j].tolist()[0:])\n",
    "        cumulative_rows.append(cumulative_row)\n",
    "    # Create a new DataFrame from the cumulative rows list\n",
    "    my_train_X = pd.DataFrame(cumulative_rows)\n",
    "    my_train_X.reset_index(drop=True, inplace=True)\n",
    "    my_train_X.to_csv('my_train_X.csv')\n",
    "    my_train_X\n",
    "\n",
    "    cumulative_rows = []\n",
    "    for i in range(0, len(merged_test_daily_return_x)-days, 1):\n",
    "        group = merged_test_daily_return_x.iloc[i:i+days]\n",
    "        cumulative_row = group.iloc[0].tolist()\n",
    "        for j in range(1, len(group)):\n",
    "            cumulative_row.extend(group.iloc[j].tolist()[0:])\n",
    "        cumulative_rows.append(cumulative_row)\n",
    "    # Create a new DataFrame from the cumulative rows list\n",
    "    my_test_X = pd.DataFrame(cumulative_rows)\n",
    "    my_test_X.reset_index(drop=True, inplace=True)\n",
    "    my_test_X.to_csv('my_test_X.csv')\n",
    "    my_test_X\n",
    "\n",
    "    merged_train_daily_return_y = merged_train_daily_return.iloc[days:, 5:10]\n",
    "\n",
    "    merged_train_daily_return_y.reset_index(drop=True, inplace=True)\n",
    "    my_train_Y = merged_train_daily_return_y\n",
    "    my_train_Y.to_csv('my_train_Y.csv')\n",
    "    my_train_Y\n",
    "    def assign_max(row):\n",
    "        max_value = row.max()  # Find the maximum value in the row\n",
    "        return [1 if val == max_value else 0 for val in row]\n",
    "\n",
    "    # Apply the function to each row of the DataFrame and expand the result into separate columns\n",
    "    max_df = my_train_Y.apply(assign_max, axis=1, result_type='expand')\n",
    "    max_df.columns = ['a2c', 'ppo', 'ddpg', 'td3', 'sac']\n",
    "\n",
    "    # max_df.to_csv('all_agents_predict_rank.csv')\n",
    "    max_df.to_csv('my_train_Y_zero_one.csv')\n",
    "    my_train_Y = max_df\n",
    "    my_train_Y\n",
    "    merged_test_daily_return_y = merged_test_daily_return.iloc[days:, 5:10]\n",
    "\n",
    "    merged_test_daily_return_y.reset_index(drop=True, inplace=True)\n",
    "    my_test_Y = merged_test_daily_return_y\n",
    "    my_test_Y.to_csv('my_test_Y.csv')\n",
    "    my_test_Y\n",
    "\n",
    "    def assign_max(row):\n",
    "        max_value = row.max()  # Find the maximum value in the row\n",
    "        return [1 if val == max_value else 0 for val in row]\n",
    "    max_df = my_test_Y.apply(assign_max, axis=1, result_type='expand')\n",
    "    max_df.columns = ['a2c', 'ppo', 'ddpg', 'td3', 'sac']\n",
    "\n",
    "    max_df.to_csv('my_test_Y_zero_one.csv')\n",
    "    my_test_Y=max_df\n",
    "    my_test_Y\n",
    "    print('train_X shape: ',my_train_X.shape)\n",
    "    print('train_Y shape: ',my_train_Y.shape)\n",
    "    print('test_X shape: ',my_test_X.shape)\n",
    "    print('test_Y shape: ',my_test_Y.shape)\n",
    "\n",
    "\n",
    "    # Load the dataset files\n",
    "    train_X = my_train_X\n",
    "    test_X = my_test_X\n",
    "\n",
    "    robust_scaler = RobustScaler()\n",
    "    train_X_scaled = robust_scaler.fit_transform(train_X)\n",
    "    test_X_scaled = robust_scaler.transform(test_X)\n",
    "\n",
    "\n",
    "    # Convert back to DataFrame and save\n",
    "    train_X_scaled_df = pd.DataFrame(train_X_scaled, columns=train_X.columns)\n",
    "    test_X_scaled_df = pd.DataFrame(test_X_scaled, columns=test_X.columns)\n",
    "\n",
    "    my_train_X = train_X_scaled_df\n",
    "    my_test_X = test_X_scaled_df\n",
    "\n",
    "    if neural_network == 'lstm':\n",
    "        \n",
    "        # Convert DataFrame columns to NumPy arrays\n",
    "        my_train_X_array = my_train_X.to_numpy()\n",
    "        my_test_X_array = my_test_X.to_numpy()\n",
    "        my_train_Y_array = my_train_Y.to_numpy()\n",
    "        my_test_Y_array = my_test_Y.to_numpy()\n",
    "\n",
    "        # Split training data into training and validation sets\n",
    "        train_X, val_X, train_Y, val_Y = train_test_split(my_train_X_array, my_train_Y_array, test_size=0.2, random_state=42)\n",
    "\n",
    "        # Reshape the input data for LSTM\n",
    "        train_X_reshaped = train_X.reshape((train_X.shape[0], 50, 1))\n",
    "        val_X_reshaped = val_X.reshape((val_X.shape[0], 50, 1))\n",
    "        test_X_reshaped = my_test_X_array.reshape((my_test_X_array.shape[0], 50, 1))\n",
    "\n",
    "        # Define the LSTM model\n",
    "        model = Sequential()\n",
    "        model.add(LSTM(200, input_shape=(50, 1)))  # LSTM layer with 200 units\n",
    "        model.add(Dense(1500, activation='relu'))  # Dense layer with 1500 units and ReLU activation\n",
    "        model.add(Dense(5, activation='softmax'))  # Output layer with 5 units for 5 classes and softmax activation\n",
    "\n",
    "        # Compile the model\n",
    "        model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "        model.summary()  # Print model summary\n",
    "\n",
    "        # Define ModelCheckpoint callback to save the best model based on validation accuracy\n",
    "        checkpoint = ModelCheckpoint('best_model_lstm.keras', monitor='val_accuracy', save_best_only=True, mode='max', verbose=1)\n",
    "\n",
    "        # Train the model with validation data\n",
    "        history = model.fit(train_X_reshaped, train_Y, epochs=50, batch_size=256, validation_data=(val_X_reshaped, val_Y), callbacks=[checkpoint])\n",
    "\n",
    "        # Evaluate the model on test data\n",
    "        test_loss, test_accuracy = model.evaluate(test_X_reshaped, my_test_Y_array)\n",
    "        print(f'Test Loss: {test_loss}')\n",
    "        print(f'Test Accuracy: {test_accuracy}')\n",
    "\n",
    "        # Predictions and saving to CSV\n",
    "        predictions = model.predict(test_X_reshaped)\n",
    "        final_prediction_shahin = pd.DataFrame(predictions)\n",
    "        final_prediction_shahin.to_csv('main_proposed_method_prediction.csv')\n",
    "\n",
    "        predicted_classes = np.argmax(predictions, axis=1)\n",
    "        df_predicted = pd.DataFrame({'Predicted_Class': predicted_classes})\n",
    "        df_predicted.to_csv('main_proposed_method_predicted_classes.csv', index=False)\n",
    "\n",
    "        # One-hot encoded predictions\n",
    "        num_classes = 5\n",
    "        one_hot_predictions = np.eye(num_classes)[predicted_classes]\n",
    "        np.savetxt('main_proposed_method_one_hot_predictions.csv', one_hot_predictions, delimiter=',')\n",
    "\n",
    "        # Plotting training history\n",
    "        plt.figure(figsize=(25, 10))\n",
    "\n",
    "        # Accuracy plot\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "        plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "        plt.title('Model Accuracy')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.legend()\n",
    "\n",
    "        # Loss plot\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(history.history['loss'], label='Train Loss')\n",
    "        plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "        plt.title('Model Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "\n",
    "        # Save the figure before showing\n",
    "        plt.savefig('./results/training_history.pdf')\n",
    "\n",
    "        # Print predicted classes (optional)\n",
    "        predicted_classes\n",
    "\n",
    "    if neural_network == 'mlp':\n",
    "\n",
    "        # Split data into training and validation sets\n",
    "        train_X, val_X, train_Y, val_Y = train_test_split(my_train_X, my_train_Y, test_size=0.2, random_state=42)\n",
    "    \n",
    "        # Define the MLP model\n",
    "        model = Sequential()\n",
    "        model.add(Dense(2000, input_dim=50, activation='relu'))\n",
    "        model.add(Dropout(0.4))\n",
    "        model.add(Dense(5, activation='softmax'))\n",
    "        # Compile the model\n",
    "        model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "        model.summary()\n",
    "\n",
    "        # Define ModelCheckpoint callback to save the best model\n",
    "        checkpoint = ModelCheckpoint('best_model_mlp.keras', monitor='val_accuracy', save_best_only=True, mode='max', verbose=1)\n",
    "\n",
    "        # Train the model with validation data\n",
    "        history = model.fit(train_X, train_Y, epochs=2000, batch_size=2048, validation_data=(val_X, val_Y), callbacks=[checkpoint])\n",
    "\n",
    "        # Load the best model saved by ModelCheckpoint\n",
    "        best_model = load_model('best_model_mlp.keras')\n",
    "\n",
    "        # Evaluate the best model on test data\n",
    "        test_loss, test_accuracy = best_model.evaluate(my_test_X, my_test_Y)\n",
    "        print(f'Test Loss: {test_loss}')\n",
    "        print(f'Test Accuracy: {test_accuracy}')\n",
    "\n",
    "        # Predictions and saving to CSV\n",
    "        predictions = best_model.predict(my_test_X)\n",
    "        final_prediction_shahin = pd.DataFrame(predictions)\n",
    "        final_prediction_shahin.to_csv('main_proposed_method_prediction.csv')\n",
    "\n",
    "        predicted_classes = np.argmax(predictions, axis=1)\n",
    "        df_predicted = pd.DataFrame({'Predicted_Class': predicted_classes})\n",
    "        df_predicted.to_csv('main_proposed_method_predicted_classes.csv', index=False)\n",
    "\n",
    "        num_classes = 5\n",
    "        one_hot_predictions = np.eye(num_classes)[predicted_classes]\n",
    "        np.savetxt('main_proposed_method_one_hot_predictions.csv', one_hot_predictions, delimiter=',')\n",
    "\n",
    "        predicted_classes\n",
    "\n",
    "\n",
    "    # Assuming my_train_X, my_train_Y, my_test_X, and my_test_Y are already defined and in DataFrame format\n",
    "    if neural_network == 'cnn':\n",
    "\n",
    "        # Convert DataFrame to NumPy array and reshape data to add a channel dimension\n",
    "        my_train_X = my_train_X.to_numpy().reshape(-1, 50, 1)\n",
    "        my_test_X = my_test_X.to_numpy().reshape(-1, 50, 1)\n",
    "\n",
    "        # Convert labels to NumPy array if they are in DataFrame format\n",
    "        my_train_Y = my_train_Y.to_numpy()\n",
    "        my_test_Y = my_test_Y.to_numpy()\n",
    "\n",
    "        # Split data into training and validation sets\n",
    "        train_X, val_X, train_Y, val_Y = train_test_split(my_train_X, my_train_Y, test_size=0.2, random_state=42)\n",
    "    \n",
    "        # Define the CNN model\n",
    "        model = Sequential()\n",
    "        model.add(Conv1D(64, 2, activation='relu', input_shape=(50, 1)))\n",
    "        model.add(MaxPooling1D(2))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(1024, activation='relu'))\n",
    "        model.add(Dropout(0.8))\n",
    "        model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "        # Compile the model\n",
    "        model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "        model.summary()\n",
    "\n",
    "        # Define ModelCheckpoint callback to save the best model\n",
    "        checkpoint = ModelCheckpoint('best_model_cnn.keras', monitor='val_accuracy', save_best_only=True, mode='max', verbose=1)\n",
    "\n",
    "        # Train the model with validation data\n",
    "        history = model.fit(train_X, train_Y, epochs=100, batch_size=1024, validation_data=(val_X, val_Y), callbacks=[checkpoint])\n",
    "\n",
    "        # Load the best model saved by ModelCheckpoint\n",
    "        best_model = load_model('best_model_cnn.keras')\n",
    "\n",
    "        # Evaluate the best model on test data\n",
    "        test_loss, test_accuracy = best_model.evaluate(my_test_X, my_test_Y)\n",
    "        print(f'Test Loss: {test_loss}')\n",
    "        print(f'Test Accuracy: {test_accuracy}')\n",
    "\n",
    "        # Predictions and saving to CSV\n",
    "        predictions = best_model.predict(my_test_X)\n",
    "        final_prediction_shahin = pd.DataFrame(predictions)\n",
    "        final_prediction_shahin.to_csv('main_proposed_method_prediction.csv')\n",
    "\n",
    "        predicted_classes = np.argmax(predictions, axis=1)\n",
    "        df_predicted = pd.DataFrame({'Predicted_Class': predicted_classes})\n",
    "        df_predicted.to_csv('main_proposed_method_predicted_classes.csv', index=False)\n",
    "\n",
    "        num_classes = 5\n",
    "        one_hot_predictions = np.eye(num_classes)[predicted_classes]\n",
    "        np.savetxt('main_proposed_method_one_hot_predictions.csv', one_hot_predictions, delimiter=',')\n",
    "\n",
    "        # Plotting training history\n",
    "        plt.figure(figsize=(25, 10))\n",
    "\n",
    "        # Accuracy plot\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "        plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "        plt.title('Model Accuracy')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.legend()\n",
    "\n",
    "        # Loss plot\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(history.history['loss'], label='Train Loss')\n",
    "        plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "        plt.title('Model Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "\n",
    "        # Save the figure before showing\n",
    "        plt.savefig('./results/training_history.pdf')\n",
    "\n",
    "    # Ensure the results directory exists\n",
    "    os.makedirs('./results/model_mlp_result/', exist_ok=True)\n",
    "\n",
    "    # Assuming my_train_X, my_train_Y, my_test_X, and my_test_Y are already defined and in DataFrame format\n",
    "    if neural_network == 'dense_bn':\n",
    "\n",
    "        # Convert DataFrame to NumPy array\n",
    "        my_train_X = my_train_X.to_numpy()\n",
    "        my_test_X = my_test_X.to_numpy()\n",
    "\n",
    "        # Convert labels to NumPy array if they are in DataFrame format\n",
    "        my_train_Y = my_train_Y.to_numpy()\n",
    "        my_test_Y = my_test_Y.to_numpy()\n",
    "\n",
    "        # Split data into training and validation sets\n",
    "        train_X, val_X, train_Y, val_Y = train_test_split(my_train_X, my_train_Y, test_size=0.2, random_state=42)\n",
    "    \n",
    "        # Define the Dense model with Batch Normalization\n",
    "        model = Sequential()\n",
    "        model.add(Dense(256, input_dim=my_train_X.shape[1], activation='relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.7))\n",
    "        model.add(Dense(64, activation='relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.7))\n",
    "        model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "        # Compile the model\n",
    "        model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "        model.summary()\n",
    "\n",
    "        # Define ModelCheckpoint callback to save the best model\n",
    "        checkpoint = ModelCheckpoint('best_model_dense_bn.keras', monitor='val_accuracy', save_best_only=True, mode='max', verbose=1)\n",
    "        \n",
    "        # Train the model with validation data\n",
    "        history = model.fit(train_X, train_Y, epochs=1500, batch_size=1024, validation_data=(val_X, val_Y), callbacks=[checkpoint])\n",
    "\n",
    "        # Load the best model saved by ModelCheckpoint\n",
    "        best_model = load_model('best_model_dense_bn.keras')\n",
    "\n",
    "        # Evaluate the best model on test data\n",
    "        test_loss, test_accuracy = best_model.evaluate(my_test_X, my_test_Y)\n",
    "        print(f'Test Loss: {test_loss}')\n",
    "        print(f'Test Accuracy: {test_accuracy}')\n",
    "\n",
    "        # Predictions and saving to CSV\n",
    "        predictions = best_model.predict(my_test_X)\n",
    "        final_prediction_shahin = pd.DataFrame(predictions)\n",
    "        final_prediction_shahin.to_csv('main_proposed_method_prediction.csv', index=False)\n",
    "\n",
    "        predicted_classes = np.argmax(predictions, axis=1)\n",
    "        df_predicted = pd.DataFrame({'Predicted_Class': predicted_classes})\n",
    "        df_predicted.to_csv('main_proposed_method_predicted_classes.csv', index=False)\n",
    "\n",
    "        num_classes = 5\n",
    "        one_hot_predictions = np.eye(num_classes)[predicted_classes]\n",
    "        np.savetxt('main_proposed_method_one_hot_predictions.csv', one_hot_predictions, delimiter=',')\n",
    "\n",
    "        # Plotting training history\n",
    "        plt.figure(figsize=(25, 10))\n",
    "\n",
    "        # Accuracy plot\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "        plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "        plt.title('Model Accuracy')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.legend()\n",
    "\n",
    "        # Loss plot\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(history.history['loss'], label='Train Loss')\n",
    "        plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "        plt.title('Model Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "\n",
    "        # Save the figure before showing\n",
    "        plt.savefig('./results/training_history.pdf')\n",
    "\n",
    "    # Simple CNN model for the CIFAR-10 Dataset with improvements\n",
    "\n",
    "    # Ensure the results directory exists\n",
    "    os.makedirs('./results/model_cnn_result/', exist_ok=True)\n",
    "\n",
    "    # Assuming my_train_X, my_train_Y, my_test_X, and my_test_Y are already defined and in DataFrame format\n",
    "    if neural_network == 'conv2d':\n",
    "        # Convert DataFrame to NumPy array\n",
    "        my_train_X = my_train_X.values\n",
    "        reshaped_array = my_train_X.reshape((3015, 5, 10, 1))\n",
    "        my_train_X = reshaped_array\n",
    "\n",
    "        my_test_X = my_test_X.values\n",
    "        reshaped_array = my_test_X.reshape((745, 5, 10, 1))\n",
    "        my_test_X = reshaped_array\n",
    "\n",
    "        # Convert labels to NumPy array if they are in DataFrame format\n",
    "        my_train_Y = my_train_Y.to_numpy()\n",
    "        my_test_Y = my_test_Y.to_numpy()\n",
    "\n",
    "        # Split data into training and validation sets\n",
    "        train_X, val_X, train_Y, val_Y = train_test_split(my_train_X, my_train_Y, test_size=0.2, random_state=42)\n",
    "\n",
    "        # Fix random seed for reproducibility\n",
    "        seed = 7\n",
    "        np.random.seed(seed)\n",
    "\n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(256, (5, 5), input_shape=(5, 10, 1), activation='relu', padding='same'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling2D(pool_size=(1, 1)))\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(512, activation='relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.5))\n",
    "\n",
    "        model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "        # Compile model\n",
    "        epoch = 1000\n",
    "        lrate = 0.001\n",
    "        decay = lrate / epoch\n",
    "        sgd = SGD(learning_rate=lrate, momentum=0.9, decay=decay, nesterov=True)\n",
    "        model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "        print(model.summary())\n",
    "\n",
    "        # Define ModelCheckpoint callback to save the best model\n",
    "        checkpoint = ModelCheckpoint('best_model_cnn.keras', monitor='val_accuracy', save_best_only=True, mode='max', verbose=1)\n",
    "        \n",
    "        # Train the model with validation data\n",
    "        history = model.fit(train_X, train_Y, epochs=epoch, batch_size=1024, validation_data=(val_X, val_Y), callbacks=[checkpoint])\n",
    "\n",
    "        # Load the best model saved by ModelCheckpoint\n",
    "        best_model = load_model('best_model_cnn.keras')\n",
    "\n",
    "        # Evaluate the best model on test data\n",
    "        test_loss, test_accuracy = best_model.evaluate(my_test_X, my_test_Y)\n",
    "        print(f'Test Loss: {test_loss}')\n",
    "        print(f'Test Accuracy: {test_accuracy}')\n",
    "\n",
    "        # Predictions and saving to CSV\n",
    "        predictions = best_model.predict(my_test_X)\n",
    "        final_prediction_shahin = pd.DataFrame(predictions)\n",
    "        final_prediction_shahin.to_csv('final_prediction_shahin.csv', index=False)\n",
    "\n",
    "        predicted_classes = np.argmax(predictions, axis=1)\n",
    "        df_predicted = pd.DataFrame({'Predicted_Class': predicted_classes})\n",
    "        df_predicted.to_csv('final_predicted_classes.csv', index=False)\n",
    "\n",
    "        num_classes = 5\n",
    "        one_hot_predictions = np.eye(num_classes)[predicted_classes]\n",
    "        np.savetxt('final_one_hot_predictions.csv', one_hot_predictions, delimiter=',')\n",
    "\n",
    "        # Plotting training history\n",
    "        plt.figure(figsize=(25, 10))\n",
    "\n",
    "        # Accuracy plot\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "        plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "        plt.title('Model Accuracy')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.legend()\n",
    "\n",
    "        # Loss plot\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(history.history['loss'], label='Train Loss')\n",
    "        plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "        plt.title('Model Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "\n",
    "        # Save the figure before showing\n",
    "        plt.savefig('./results/training_history.pdf')\n",
    "\n",
    "    if neural_network == 'dense_bn':\n",
    "        my_dic_name = 'model_mlp_result'\n",
    "    if neural_network == 'cnn':\n",
    "        my_dic_name = 'model_cnn_result'\n",
    "    if neural_network == 'lstm':\n",
    "        my_dic_name = 'model_lstm_result'\n",
    "    # Assuming my_train_X, my_train_Y, my_test_X, and my_test_Y are already defined and in DataFrame format\n",
    "    # Assuming original column names are stored in a list original_columns\n",
    "\n",
    "    # Ensure the results directory exists\n",
    "    os.makedirs(f'./results/{my_dic_name}/', exist_ok=True)\n",
    "\n",
    "    # Load the best model saved by ModelCheckpoint\n",
    "    best_model = load_model('best_model_dense_bn.keras')\n",
    "\n",
    "    # Predictions\n",
    "    predictions = best_model.predict(my_test_X)\n",
    "    predicted_classes = np.argmax(predictions, axis=1)\n",
    "    true_classes = np.argmax(my_test_Y, axis=1)\n",
    "\n",
    "    # Evaluate the model on test data\n",
    "    test_loss, test_accuracy = best_model.evaluate(my_test_X, my_test_Y)\n",
    "    print(f'Test Loss: {test_loss}')\n",
    "    print(f'Test Accuracy: {test_accuracy}')\n",
    "\n",
    "    # Confusion Matrix\n",
    "    conf_matrix = confusion_matrix(true_classes, predicted_classes)\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(conf_matrix)\n",
    "\n",
    "    # Plotting Confusion Matrix\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.savefig(f'./results/{my_dic_name}/confusion_matrix.pdf')\n",
    "\n",
    "    # Classification Report with new class names\n",
    "    class_names = ['A2C', 'PPO', 'DDPG', 'TD3', 'SAC']\n",
    "    class_report = classification_report(true_classes, predicted_classes, target_names=class_names)\n",
    "    print(\"Classification Report:\")\n",
    "    print(class_report)\n",
    "\n",
    "    # Check for class imbalance\n",
    "    train_class_counts = np.sum(my_train_Y, axis=0)\n",
    "    test_class_counts = np.sum(my_test_Y, axis=0)\n",
    "    print(f\"Train class distribution: {train_class_counts}\")\n",
    "    print(f\"Test class distribution: {test_class_counts}\")\n",
    "\n",
    "    # Generate original columns based on the shape of my_train_X\n",
    "    original_columns = [f'feature_{i}' for i in range(my_train_X.shape[1])]\n",
    "\n",
    "    # Create DataFrames\n",
    "    train_X_df = pd.DataFrame(my_train_X, columns=original_columns)\n",
    "    test_X_df = pd.DataFrame(my_test_X, columns=original_columns)\n",
    "\n",
    "    # Check for missing values\n",
    "    print(\"Checking for missing values in train data...\")\n",
    "    print(train_X_df.isnull().sum())\n",
    "    print(\"Checking for missing values in test data...\")\n",
    "    print(test_X_df.isnull().sum())\n",
    "\n",
    "    # Investigate features with high variance\n",
    "    print(\"Feature variance:\")\n",
    "    print(train_X_df.var())\n",
    "\n",
    "    # Remove features with very high variance if needed\n",
    "    high_variance_features = train_X_df.columns[train_X_df.var() > 1.5]  # Example threshold\n",
    "    my_train_X_reduced = train_X_df.drop(columns=high_variance_features).to_numpy()\n",
    "    my_test_X_reduced = test_X_df.drop(columns=high_variance_features).to_numpy()\n",
    "\n",
    "    print(\"Reduced feature set:\")\n",
    "    print(my_train_X_reduced.shape)\n",
    "\n",
    "    # Saving data to CSV files\n",
    "    train_X_df.to_csv(f'./results/{my_dic_name}/train_X.csv', index=False)\n",
    "    test_X_df.to_csv(f'./results/{my_dic_name}/test_X.csv', index=False)\n",
    "    pd.DataFrame(my_train_Y, columns=class_names).to_csv(f'./results/{my_dic_name}/train_Y.csv', index=False)\n",
    "    pd.DataFrame(my_test_Y, columns=class_names).to_csv(f'./results/{my_dic_name}/test_Y.csv', index=False)\n",
    "\n",
    "    # ROC Curve plotting and saving data\n",
    "    n_classes = my_test_Y.shape[1]\n",
    "    roc_data = pd.DataFrame()\n",
    "    auc_values = []\n",
    "\n",
    "    # Compute ROC curve and ROC area for each class\n",
    "    for i in range(n_classes):\n",
    "        fpr, tpr, _ = roc_curve(my_test_Y[:, i], predictions[:, i])\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        auc_values.append(roc_auc)\n",
    "        \n",
    "        class_roc_data = pd.DataFrame({\n",
    "            f'False Positive Rate Class {i}': fpr,\n",
    "            f'True Positive Rate Class {i}': tpr\n",
    "        })\n",
    "        \n",
    "        # Merge class ROC data with main ROC data\n",
    "        roc_data = pd.concat([roc_data, class_roc_data], axis=1)\n",
    "\n",
    "    # Save ROC curve data to CSV\n",
    "    roc_data.to_csv(f'./results/{my_dic_name}/roc_curve_data.csv', index=False)\n",
    "\n",
    "    # Save AUC values to CSV\n",
    "    auc_df = pd.DataFrame({'Class': class_names, 'AUC': auc_values})\n",
    "    auc_df.to_csv(f'./results/{my_dic_name}/auc_values.csv', index=False)\n",
    "\n",
    "    # Plotting ROC curve for each class\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    for i in range(n_classes):\n",
    "        plt.plot(roc_data[f'False Positive Rate Class {i}'], roc_data[f'True Positive Rate Class {i}'], lw=2, label=f'Class {class_names[i]} (area = {auc_values[i]:.2f})')\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.savefig(f'./results/{my_dic_name}/roc.pdf')\n",
    "\n",
    "    main_proposed_method_prediction = pd.read_csv('main_proposed_method_prediction.csv', header=0)\n",
    "    main_proposed_method_prediction = main_proposed_method_prediction.rename(columns={'0': 'a2c','1':'ppo','2':'ddpg','3':'td3','4':'sac','5':'loss_day'})\n",
    "    zeros_df = pd.DataFrame(np.zeros((days, main_proposed_method_prediction.shape[1])), columns=main_proposed_method_prediction.columns)\n",
    "    zeros_df['td3']=0\n",
    "    main_proposed_method_prediction = pd.concat([zeros_df, main_proposed_method_prediction], ignore_index=True)\n",
    "\n",
    "    ranked_df = main_proposed_method_prediction\n",
    "\n",
    "    def assign_max(row):\n",
    "        max_value = row.max()  # Find the maximum value in the row\n",
    "        if max_value == 0:\n",
    "            return [0] + [0] * (len(row) - 1)\n",
    "        else:\n",
    "            # Assign 1 to the max value and 0 to others\n",
    "            return [1 if val == max_value else 0 for val in row]\n",
    "\n",
    "\n",
    "    # Apply the function to each row of the DataFrame and expand the result into separate columns\n",
    "    max_df = ranked_df.apply(assign_max, axis=1, result_type='expand')\n",
    "    max_df.columns = ['a2c', 'ppo', 'ddpg', 'td3', 'sac']\n",
    "\n",
    "    max_df.to_csv('qwerty.csv')\n",
    "    max_df\n",
    "    a2c_test_weights2 = pd.read_csv('a2c_test_weights.csv')\n",
    "    ppo_test_weights2 = pd.read_csv('ppo_test_weights.csv')\n",
    "    ddpg_test_weights2 = pd.read_csv('ddpg_test_weights.csv')\n",
    "    td3_test_weights2 = pd.read_csv('td3_test_weights.csv')\n",
    "    sac_test_weights2 = pd.read_csv('sac_test_weights.csv')\n",
    "\n",
    "    a2c_test_weights = a2c_test_weights2.drop(columns=['date'])\n",
    "    ppo_test_weights = ppo_test_weights2.drop(columns=['date'])\n",
    "    ddpg_test_weights = ddpg_test_weights2.drop(columns=['date'])\n",
    "    td3_test_weights = td3_test_weights2.drop(columns=['date'])\n",
    "    sac_test_weights = sac_test_weights2.drop(columns=['date'])\n",
    "\n",
    "    a2c_test_weights.reset_index(drop=True, inplace=True)\n",
    "    ppo_test_weights.reset_index(drop=True, inplace=True)\n",
    "    ddpg_test_weights.reset_index(drop=True, inplace=True)\n",
    "    td3_test_weights.reset_index(drop=True, inplace=True)\n",
    "    sac_test_weights.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    a2c_test_weights\n",
    "    a2c_merged_df = pd.concat([a2c_test_weights, max_df.iloc[:, 0]], axis=1)\n",
    "    ppo_merged_df = pd.concat([ppo_test_weights, max_df.iloc[:, 1]], axis=1)\n",
    "    ddpg_merged_df = pd.concat([ddpg_test_weights, max_df.iloc[:, 2]], axis=1)\n",
    "    td3_merged_df = pd.concat([td3_test_weights, max_df.iloc[:, 3]], axis=1)\n",
    "    sac_merged_df = pd.concat([sac_test_weights, max_df.iloc[:, 4]], axis=1)\n",
    "\n",
    "    td3_merged_df\n",
    "    a2c_merged_df.iloc[:, 0:]\n",
    "    a2c_merged_df.iloc[:, :-1] *= a2c_merged_df.iloc[:, -1].values[:, None]\n",
    "    ppo_merged_df.iloc[:, :-1] *= ppo_merged_df.iloc[:, -1].values[:, None]\n",
    "    ddpg_merged_df.iloc[:, :-1] *= ddpg_merged_df.iloc[:, -1].values[:, None]\n",
    "    td3_merged_df.iloc[:, :-1] *= td3_merged_df.iloc[:, -1].values[:, None]\n",
    "    sac_merged_df.iloc[:, :-1] *= sac_merged_df.iloc[:, -1].values[:, None]\n",
    "\n",
    "    a2c_merged_df = a2c_merged_df.drop(columns=['a2c'])\n",
    "    ppo_merged_df = ppo_merged_df.drop(columns=['ppo'])\n",
    "    ddpg_merged_df = ddpg_merged_df.drop(columns=['ddpg'])\n",
    "    td3_merged_df = td3_merged_df.drop(columns=['td3'])\n",
    "    sac_merged_df = sac_merged_df.drop(columns=['sac'])\n",
    "\n",
    "\n",
    "    a2c_merged_df\n",
    "    dfs = [a2c_merged_df, ppo_merged_df, ddpg_merged_df, td3_merged_df, sac_merged_df]\n",
    "\n",
    "    # Extract columns except the first column (\"date\")\n",
    "    cols_to_sum = dfs[0].columns[0:]\n",
    "    # Initialize an empty DataFrame to store the summed values\n",
    "    above_sum_df6 = pd.DataFrame(columns=cols_to_sum)\n",
    "    # Iterate over each DataFrame and sum corresponding columns\n",
    "    for df in dfs:\n",
    "        above_sum_df6 = above_sum_df6.add(df[cols_to_sum], fill_value=0)\n",
    "    # Add the \"date\" column from df1 to the summed values DataFrame\n",
    "\n",
    "    above_sum_df6.to_csv('./results/main_proposed_method/test_weights.csv')\n",
    "    above_sum_df6\n",
    "    percent_change_df = above_sum_df6.pct_change()\n",
    "    percent_change_df.to_csv('./results/main_proposed_method/percent_change.csv')\n",
    "    %store -r filtered_stocks\n",
    "    start_date = '2021-03-29'\n",
    "    end_date = '2024-03-28'\n",
    "    filtered_df = df_close_full_stocks[(df_close_full_stocks['date'] < end_date) & (df_close_full_stocks['date'] >= start_date)]\n",
    "    filtered_df.reset_index(drop=True, inplace=True)\n",
    "    columns_to_drop = [col for col in filtered_df if col not in filtered_stocks]\n",
    "    df_kept = filtered_df.drop(columns=columns_to_drop)\n",
    "    df1 = df_kept.reindex(sorted(df_kept.columns), axis=1)\n",
    "    test_close_normal = df1.pct_change()\n",
    "    hoseini_algorithm = above_sum_df6\n",
    "    hosini_daily_return = test_close_normal * hoseini_algorithm\n",
    "    hosini_daily_return.to_csv('hosini_daily_return.csv', index=False)\n",
    "    hosini_daily_return\n",
    "\n",
    "    def load_csv_files(filenames):\n",
    "        dataframes = {}\n",
    "        for name, filename in filenames.items():\n",
    "            df = pd.read_csv(filename)\n",
    "            if 'Unnamed: 0' in df.columns:\n",
    "                df = df.drop(columns=['Unnamed: 0'])\n",
    "            dataframes[name] = df\n",
    "        return dataframes\n",
    "\n",
    "    # Dictionary of filenames\n",
    "    filenames = {\n",
    "        'a2c_train_daily_return': 'a2c_train_daily_return.csv',\n",
    "        'ppo_train_daily_return': 'ppo_train_daily_return.csv',\n",
    "        'ddpg_train_daily_return': 'ddpg_train_daily_return.csv',\n",
    "        'td3_train_daily_return': 'td3_train_daily_return.csv',\n",
    "        'sac_train_daily_return': 'sac_train_daily_return.csv',\n",
    "        'a2c_test_returns': 'a2c_test_daily_return.csv',\n",
    "        'ppo_test_returns': 'ppo_test_daily_return.csv',\n",
    "        'ddpg_test_returns': 'ddpg_test_daily_return.csv',\n",
    "        'td3_test_returns': 'td3_test_daily_return.csv',\n",
    "        'sac_test_returns': 'sac_test_daily_return.csv',\n",
    "        'proposed_method1_test_daily_return': 'proposed_method1_test_daily_return.csv',\n",
    "    }\n",
    "\n",
    "    # Load the data\n",
    "    dataframes = load_csv_files(filenames)\n",
    "\n",
    "    # Access individual DataFrames\n",
    "    a2c_train_daily_return = dataframes['a2c_train_daily_return']\n",
    "    ppo_train_daily_return = dataframes['ppo_train_daily_return']\n",
    "    ddpg_train_daily_return = dataframes['ddpg_train_daily_return']\n",
    "    td3_train_daily_return = dataframes['td3_train_daily_return']\n",
    "    sac_train_daily_return = dataframes['sac_train_daily_return']\n",
    "    a2c_test_returns = dataframes['a2c_test_returns']\n",
    "    ppo_test_returns = dataframes['ppo_test_returns']\n",
    "    ddpg_test_returns = dataframes['ddpg_test_returns']\n",
    "    td3_test_returns = dataframes['td3_test_returns']\n",
    "    sac_test_returns = dataframes['sac_test_returns']\n",
    "    proposed_method1_test_daily_return = dataframes['proposed_method1_test_daily_return']\n",
    "\n",
    "    %store a2c_train_daily_return\n",
    "    %store ppo_train_daily_return\n",
    "    %store ddpg_train_daily_return\n",
    "    %store td3_train_daily_return\n",
    "    %store sac_train_daily_return\n",
    "\n",
    "    %store a2c_test_returns\n",
    "    %store ppo_test_returns\n",
    "    %store ddpg_test_returns\n",
    "    %store td3_test_returns\n",
    "    %store sac_test_returns\n",
    "\n",
    "    %store proposed_method1_test_daily_return\n",
    "\n",
    "    %store -r prices_train_df\n",
    "    %store -r prices_test_df\n",
    "\n",
    "    a2c_train_cum_returns = (\n",
    "        1 + a2c_train_daily_return.reset_index(drop=True).set_index(['date'])).cumprod()\n",
    "    a2c_train_cum_returns = a2c_train_cum_returns['daily_return']\n",
    "    a2c_train_cum_returns.name = 'Portfolio 1: A2C Model'\n",
    "    ppo_train_cum_returns = (\n",
    "        1 + ppo_train_daily_return.reset_index(drop=True).set_index(['date'])).cumprod()\n",
    "    ppo_train_cum_returns = ppo_train_cum_returns['daily_return']\n",
    "    ppo_train_cum_returns.name = 'Portfolio 2: PPO Model'\n",
    "    ddpg_train_cum_returns = (\n",
    "        1 + ddpg_train_daily_return.reset_index(drop=True).set_index(['date'])).cumprod()\n",
    "    ddpg_train_cum_returns = ddpg_train_cum_returns['daily_return']\n",
    "    ddpg_train_cum_returns.name = 'Portfolio 3: DDPG Model'\n",
    "    td3_train_cum_returns = (\n",
    "        1 + td3_train_daily_return.reset_index(drop=True).set_index(['date'])).cumprod()\n",
    "    td3_train_cum_returns = td3_train_cum_returns['daily_return']\n",
    "    td3_train_cum_returns.name = 'Portfolio 4: TD3 Model'\n",
    "    sac_train_cum_returns = (\n",
    "        1 + sac_train_daily_return.reset_index(drop=True).set_index(['date'])).cumprod()\n",
    "    sac_train_cum_returns = sac_train_cum_returns['daily_return']\n",
    "    sac_train_cum_returns.name = 'Portfolio 5: SAC Model'\n",
    "    date_list = list(ddpg_train_cum_returns.index)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(25, 10))\n",
    "\n",
    "    a2c_train_cum_returns.plot(ax=ax, color='gray', alpha=0.4)\n",
    "    ppo_train_cum_returns.plot(ax=ax, color='green', alpha=0.4)\n",
    "    ddpg_train_cum_returns.plot(ax=ax, color='purple', alpha=0.4)\n",
    "    td3_train_cum_returns.plot(ax=ax, color='red', alpha=0.4)\n",
    "    sac_train_cum_returns.plot(ax=ax, color='blue', alpha=0.4)\n",
    "    plt.legend(loc=\"best\", fontsize=19, handlelength=4,\n",
    "            handleheight=2, handletextpad=2)\n",
    "    plt.grid(True)\n",
    "    ax.set_ylabel(\"cummulative return\", fontsize=30)\n",
    "    ax.set_xlabel(\"Date\", fontsize=28)\n",
    "    ax.tick_params(axis='x', labelsize=25)\n",
    "    ax.tick_params(axis='y', labelsize=25)\n",
    "    ax.set_title(\n",
    "        \"Backtest based on the data from 2009-03-20 to 2021-03-26\", fontsize=30)\n",
    "\n",
    "    fig.savefig('results/result_train/back_test_on_train_data.pdf')\n",
    "    a2c_train_cum_returns.to_csv('results/result_train/a2c_train_cum_return.csv')\n",
    "    ppo_train_cum_returns.to_csv('results/result_train/ppo_train_cum_return.csv')\n",
    "    ddpg_train_cum_returns.to_csv('results/result_train/ddpg_train_cum_return.csv')\n",
    "    td3_train_cum_returns.to_csv('results/result_train/td3_train_cum_return.csv')\n",
    "    sac_train_cum_returns.to_csv('results/result_train/sac_train_cum_return.csv')\n",
    "\n",
    "    main_proposed_method_eachstock_return = pd.read_csv(\n",
    "        'hosini_daily_return.csv')\n",
    "    main_proposed_method_eachstock_return['daily_return'] = main_proposed_method_eachstock_return.sum(axis=1)\n",
    "\n",
    "    a2c_test_cum_returns = (1 + a2c_test_returns['daily_return']).cumprod()\n",
    "    a2c_test_cum_returns.name = 'Portfolio 1: A2C Model'\n",
    "\n",
    "    ppo_test_cum_returns = (1 + ppo_test_returns['daily_return']).cumprod()\n",
    "    ppo_test_cum_returns.name = 'Portfolio 2: PPO Model'\n",
    "\n",
    "    ddpg_test_cum_returns = (1 + ddpg_test_returns['daily_return']).cumprod()\n",
    "    ddpg_test_cum_returns.name = 'Portfolio 3: DDPG Model'\n",
    "\n",
    "    td3_test_cum_returns = (1 + td3_test_returns['daily_return']).cumprod()\n",
    "    td3_test_cum_returns.name = 'Portfolio 4: TD3 Model'\n",
    "\n",
    "    sac_test_cum_returns = (1 + sac_test_returns['daily_return']).cumprod()\n",
    "    sac_test_cum_returns.name = 'Portfolio 5: SAC Model'\n",
    "\n",
    "\n",
    "    proposed_method1_test_cum_return = (\n",
    "        1 + proposed_method1_test_daily_return['daily_return']).cumprod()\n",
    "    proposed_method1_test_cum_return.name = 'Portfolio 6: Proposed method 1'\n",
    "\n",
    "    main_proposed_method_cum_return = (\n",
    "        1 + main_proposed_method_eachstock_return['daily_return']).cumprod()\n",
    "    main_proposed_method_cum_return.name = 'Portfolio 7: Main proposed method'\n",
    "\n",
    "\n",
    "    # Plot the culmulative returns of the portfolios\n",
    "    fig, ax = plt.subplots(figsize=(25,10))\n",
    "    a2c_test_cum_returns.plot(ax=ax, color='blue', alpha=.4);\n",
    "    ppo_test_cum_returns.plot(ax=ax, color='green', alpha=.4);\n",
    "    ddpg_test_cum_returns.plot(ax=ax, color='purple', alpha=.4);\n",
    "    td3_test_cum_returns.plot(ax=ax, color='red', alpha=0.4);\n",
    "    sac_test_cum_returns.plot(ax=ax, color='darkred', alpha=0.4);\n",
    "    proposed_method1_test_cum_return.plot(ax=ax, color='darkred', alpha=0.4)\n",
    "    main_proposed_method_cum_return.plot(ax=ax, color='black', alpha=0.8)\n",
    "    plt.legend(loc=\"best\", fontsize=17, handlelength=4, handleheight=1,handletextpad=2);\n",
    "    plt.grid(True);\n",
    "    ax.set_ylabel(\"cummulative return\", fontsize = 30);\n",
    "    ax.set_xlabel(\"Date\", fontsize=28);\n",
    "    ax.set_title(\"Backtest based on the data from 2021-03-29 to 2024-03-28\", fontsize=30);\n",
    "    ax.tick_params(axis='x', labelsize=25);\n",
    "    ax.tick_params(axis='y', labelsize=25);\n",
    "\n",
    "    fig.savefig('./results/result_test/back_test_on_test_data.pdf');\n",
    "    a2c_test_cum_returns.to_csv('./results/result_test/a2c_test_cum_returns.csv')\n",
    "    ppo_test_cum_returns.to_csv('./results/result_test/ppo_test_cum_returns.csv')\n",
    "    ddpg_test_cum_returns.to_csv('./results/result_test/ddpg_test_cum_returns.csv')\n",
    "    td3_test_cum_returns.to_csv('./results/result_test/td3_test_cum_returns.csv')\n",
    "    sac_test_cum_returns.to_csv('./results/result_test/sac_test_cum_returns.csv')\n",
    "    proposed_method1_test_cum_return.to_csv(\n",
    "        './results/result_test/proposed_method1_test_cum_return.csv')\n",
    "    main_proposed_method_cum_return.to_csv(\n",
    "        './results/result_test/main_proposed_method_cum_return.csv')\n",
    "    # Define a Function for Getting the Portfolio Statistics\n",
    "\n",
    "    def portfolio_stats(portfolio_returns):\n",
    "        # Pass the returns into a dataframe\n",
    "        port_rets_df = pd.DataFrame(portfolio_returns)\n",
    "        port_rets_df = port_rets_df.reset_index()\n",
    "        port_rets_df.columns = ['date', 'daily_return']\n",
    "\n",
    "        # Use the FinRL Library to get the Portfolio Returns\n",
    "        # This makes use of the Pyfolio Library\n",
    "\n",
    "        DRL_strat = backtest_strat(port_rets_df)\n",
    "        perf_func = timeseries.perf_stats\n",
    "        perf_stats_all = perf_func(returns=DRL_strat,\n",
    "                                factor_returns=DRL_strat,\n",
    "                                positions=None, transactions=None, turnover_denom=\"AGB\")\n",
    "        perf_stats_all = pd.DataFrame(perf_stats_all)\n",
    "        perf_stats_all.columns = ['Statistic']\n",
    "        return perf_stats_all\n",
    "    # Get the Portfolio Statistics for all the portfolios\n",
    "    portfolios_returns_dict = {'A2C Model': a2c_test_returns['daily_return'],\n",
    "                            'PPO Model': ppo_test_returns['daily_return'],\n",
    "                            'DDPG Model': ddpg_test_returns['daily_return'],\n",
    "                            'TD3 Model': td3_test_returns['daily_return'],\n",
    "                            'SAC Model': sac_test_returns['daily_return'],\n",
    "                            'Proposed method1': proposed_method1_test_daily_return['daily_return'],\n",
    "                            'Main proposed method': main_proposed_method_eachstock_return['daily_return']\n",
    "                            }\n",
    "\n",
    "    portfolios_stats = pd.DataFrame()\n",
    "    for i, j in portfolios_returns_dict.items():\n",
    "        port_stats = portfolio_stats(j)\n",
    "        portfolios_stats[i] = port_stats['Statistic']\n",
    "    portfolios_stats.to_csv('./results/result_portfolios/portfolios_stats.csv')\n",
    "    portfolios_stats\n",
    "    a2c_test_cum_returns = pd.read_csv('./results/result_test/a2c_test_cum_returns.csv')\n",
    "    a2c_test_cum_returns = a2c_test_cum_returns.drop(columns=['Unnamed: 0'])\n",
    "    last_column1 = a2c_test_cum_returns.iloc[-1]\n",
    "    ppo_test_cum_returns = pd.read_csv('./results/result_test/ppo_test_cum_returns.csv')\n",
    "    ppo_test_cum_returns = ppo_test_cum_returns.drop(columns=['Unnamed: 0'])\n",
    "    last_column2 = ppo_test_cum_returns.iloc[-1]\n",
    "    ddpg_test_cum_returns = pd.read_csv('./results/result_test/ddpg_test_cum_returns.csv')\n",
    "    ddpg_test_cum_returns = ddpg_test_cum_returns.drop(columns=['Unnamed: 0'])\n",
    "    last_column3 = ddpg_test_cum_returns.iloc[-1]\n",
    "    td3_test_cum_returns = pd.read_csv('./results/result_test/td3_test_cum_returns.csv')\n",
    "    td3_test_cum_returns = td3_test_cum_returns.drop(columns=['Unnamed: 0'])\n",
    "    last_column4 = td3_test_cum_returns.iloc[-1]\n",
    "    sac_test_cum_returns = pd.read_csv('./results/result_test/sac_test_cum_returns.csv')\n",
    "    sac_test_cum_returns = sac_test_cum_returns.drop(columns=['Unnamed: 0'])\n",
    "    last_column5 = sac_test_cum_returns.iloc[-1]\n",
    "    main_proposed_method_cum_return = pd.read_csv('./results/result_test/main_proposed_method_cum_return.csv')\n",
    "    main_proposed_method_cum_return = main_proposed_method_cum_return.drop(columns=['Unnamed: 0'])\n",
    "    last_column6 = main_proposed_method_cum_return.iloc[-1]\n",
    "\n",
    "    maximum = max(last_column1[0], last_column2[0],last_column3[0],last_column4[0],last_column5[0])\n",
    "\n",
    "    if last_column6[0] > maximum:\n",
    "        best_return=False\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "lvrqTro3lhAh",
    "a3Iuv554xYFH",
    "SPEXBcm-uBJo"
   ],
   "include_colab_link": true,
   "name": "FinRL_portfolio_allocation.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
